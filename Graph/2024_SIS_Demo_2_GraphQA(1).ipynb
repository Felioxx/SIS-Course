{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43I3IWmdbtN4"
   },
   "source": [
    "Topics: Data Modelling and Search Models\n",
    "* Langsmith (for inspection and debugging)\n",
    "* Semantic model extraction (continued)\n",
    "* Graph QA using GraphCypherQAChain\n",
    "* Graph QA using Vector Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsUHp7-VRbf8"
   },
   "source": [
    "# Chapter 1:  Langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbug8ocywoO7"
   },
   "source": [
    "[Documentation](https://docs.smith.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWn48jeBOlOA"
   },
   "source": [
    "[Website](https://www.langchain.com/langsmith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaMwdPwo6hVb",
    "outputId": "e32915be-98ed-406e-bbd6-93f564b3d7c9"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTM4jeoO7J7E",
    "outputId": "63363809-2fe8-4070-8777-278e728f7398"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "HA5Nm88y6ovr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YourKey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgGWbFpq7fGw",
    "outputId": "d9902e25-16d5-4286-9102-e675298f9b62"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vrjyoGMWSxFI",
    "outputId": "e3a0cd82-e78d-4875-f861-4ab2340dcc67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Place names: Germany, Europe, Berlin\\nGeopolitical entities: Germany'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser() # https://www.restack.io/docs/langchain-knowledge-langchain-stroutputparser-guide\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "question = \"What are the place names and geopolitical entities mentioned in the context?\"\n",
    "context = \"Germany is a country in Europe and its capital is Berlin.\"\n",
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv0FvC7kT3KR"
   },
   "source": [
    "# Chapter 2: Semantic Model Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2yaxk974TjZ",
    "outputId": "33bfca12-050e-47c6-f9c4-06345e4513ed"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain-community langchain-openai langchain_experimental neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "DwOmWv9h4ZAN"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+s://f02e0524.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"w60PF-SK2gGIlDII6zZMw8XMo67mqIFSrPU54_E3AU4\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Kc7B0jrCvAZO"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HZePyU65vKrD"
   },
   "outputs": [],
   "source": [
    "# From wikipedia: https://en.wikipedia.org/wiki/M%C3%BCnster\n",
    "example_text = \"\"\"\n",
    "Münster is an independent city (Kreisfreie Stadt)\n",
    "in North Rhine-Westphalia, Germany. It is in the northern part of the state and is considered to\n",
    " be the cultural centre of the Westphalia region. It is also a state district capital. Münster was the\n",
    "  location of the Anabaptist rebellion during the Protestant Reformation and the site of the signing of the\n",
    "   Treaty of Westphalia ending the Thirty Years' War in 1648. Today, it is known as the bicycle capital of Germany.\n",
    "Münster gained the status of a Großstadt (major city) with more than 100,000 inhabitants in 1915.[4]\n",
    " As of 2014, there are 300,000[5] people living in the city, with about 61,500 students,[6]\n",
    " only some of whom are recorded in the official population statistics as having their primary residence in Münster.\n",
    " Münster is a part of the international Euregio region with more than 1,000,000 inhabitants (Enschede, Hengelo, Gronau, Osnabrück).\n",
    " Companies offering jobs in Münster include the Institute for Geoinformatics at the University of Münster,\n",
    " the Münster University of Applied Sciences, Reedu GmbH, con terra, the Deutsche Bank, IKEA, LIDL, REWE, ALDI and BASF Coatings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "nO9j2tbdvPIc"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\") # https://platform.openai.com/docs/models\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm) # documentation, see https://python.langchain.com/docs/how_to/graph_constructing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYnlGBY8vTVD",
    "outputId": "b04410a5-41cc-4969-ab43-9ce3856dd6f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Münster', type='City', properties={}), Node(id='North Rhine-Westphalia', type='State', properties={}), Node(id='Germany', type='Country', properties={}), Node(id='Westphalia', type='Region', properties={}), Node(id='Anabaptist Rebellion', type='Event', properties={}), Node(id='Protestant Reformation', type='Event', properties={}), Node(id='Treaty Of Westphalia', type='Event', properties={}), Node(id=\"Thirty Years' War\", type='Event', properties={}), Node(id='Euregio', type='Region', properties={}), Node(id='Institute For Geoinformatics At The University Of Münster', type='Organization', properties={}), Node(id='Münster University Of Applied Sciences', type='Organization', properties={}), Node(id='Reedu Gmbh', type='Organization', properties={}), Node(id='Con Terra', type='Organization', properties={}), Node(id='Deutsche Bank', type='Organization', properties={}), Node(id='Ikea', type='Organization', properties={}), Node(id='Lidl', type='Organization', properties={}), Node(id='Rewe', type='Organization', properties={}), Node(id='Aldi', type='Organization', properties={}), Node(id='Basf Coatings', type='Organization', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Münster', type='City', properties={}), target=Node(id='North Rhine-Westphalia', type='State', properties={}), type='LOCATED_IN', properties={}), Relationship(source=Node(id='Münster', type='City', properties={}), target=Node(id='Westphalia', type='Region', properties={}), type='CULTURAL_CENTRE', properties={}), Relationship(source=Node(id='Anabaptist Rebellion', type='Event', properties={}), target=Node(id='Münster', type='City', properties={}), type='LOCATION', properties={}), Relationship(source=Node(id='Treaty Of Westphalia', type='Event', properties={}), target=Node(id='Münster', type='City', properties={}), type='SIGNING_LOCATION', properties={}), Relationship(source=Node(id='Münster', type='City', properties={}), target=Node(id='Euregio', type='Region', properties={}), type='PART_OF', properties={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=example_text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "8dHnhXk84n9h"
   },
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-MZn6LV9qtB"
   },
   "source": [
    "# Chapter 3: Graph QA using GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kaQFfQ539seN"
   },
   "outputs": [],
   "source": [
    "!pip install  --quiet langchain langchain-openai langchain-community neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GolFRbIT98CE"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+s://f02e0524.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"w60PF-SK2gGIlDII6zZMw8XMo67mqIFSrPU54_E3AU4\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oO3-1C_9iV47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ORmHBZE89_9z"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"), # gpt-4o-mini\tgpt-3.5-turbo\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MlmganG9yec",
    "outputId": "c8faf5af-0a4a-4117-b9ad-89f24f5e273a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (c:City)-[:within]->(d:District {Name: 'Steinfurt'})\n",
      "RETURN c.Name\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c.Name': 'Nordwalde'}, {'c.Name': 'Lengerich'}, {'c.Name': 'Recke'}, {'c.Name': 'Ibbenbüren'}, {'c.Name': 'Emsdetten'}, {'c.Name': 'Westerkappeln'}, {'c.Name': 'Steinfurt'}, {'c.Name': 'Greven'}, {'c.Name': 'Lienen'}, {'c.Name': 'Rheine'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which cities lie within Steinfurt? Only return the Names.',\n",
       " 'result': 'Nordwalde, Lengerich, Recke, Ibbenbüren, Emsdetten, Westerkappeln, Steinfurt, Greven, Lienen, Rheine.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_1 = \"What is the population of Hessen?\"\n",
    "question_2 = \"What is the geometry of Rheinland-Pfalz?\"\n",
    "question_3 = \"What are the areas of Hessen and Niedersachen. Is the area of Hessen bigger than the area of Niedersachsen\"\n",
    "question_4 = \"Is Düsseldorf the state capital of Nordrhein-Westfalen\"\n",
    "question_5 = \"Which cities lie in the district of Steinfurt?\"\n",
    "question_6 = \"Which cities lie southern, southeastern and southwestern of Münster? The relative position is saved as a property in the touches relation. Also give me every touches relation bewteen those cities.\"\n",
    "question_7 = \"Which cities lie within Steinfurt? Only return the Names.\"\n",
    "\n",
    "chain.invoke(question_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fva46S3Zb8Rs"
   },
   "source": [
    "# Chapter 4: GraphQA using Vector Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rivtxf2dkuLF"
   },
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWZcQcLRlMPb",
    "outputId": "be55d0fd-f97f-4c91-a2f7-08fec169ecea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai wikipedia tiktoken neo4j langchain_openai langchain_community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sb4QYjYElRAq"
   },
   "outputs": [],
   "source": [
    "# https://neo4j.com/developer-blog/knowledge-graph-rag-application/\n",
    "# https://github.com/tomasonjo/blogs/blob/master/llm/devops_rag.ipynb\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+s://9df9a03f.databases.neo4j.io:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"MgDR4X6UnRMmXoJ-awLtSZKZkzY43jKpUuqnZKlnqn0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "afOIDKPImj_V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlWANUOz0C6e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jbWa90lejoVi"
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for text-embedding-ada-002 in organization org-ljbWAi0QXX8V24REYtsIGaKn on tokens per min (TPM): Limit 1000000, Requested 7514243. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneo4j_vector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jVector\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m----> 7\u001b[0m vector_index \u001b[38;5;241m=\u001b[39m \u001b[43mNeo4jVector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_existing_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_node_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#['name', 'description', 'status'], #['name', 'state_capital', 'url'],\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_node_property\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\neo4j_vector.py:1527\u001b[0m, in \u001b[0;36mNeo4jVector.from_existing_graph\u001b[1;34m(cls, embedding, node_label, embedding_node_property, text_node_properties, keyword_index_name, index_name, search_type, retrieval_query, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   1531\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: el[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: embedding}\n\u001b[0;32m   1532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m el, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data, text_embeddings)\n\u001b[0;32m   1533\u001b[0m     ]\n\u001b[0;32m   1534\u001b[0m }\n\u001b[0;32m   1536\u001b[0m store\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNWIND $data AS row \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATCH (n:`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1543\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-ada-002 in organization org-ljbWAi0QXX8V24REYtsIGaKn on tokens per min (TPM): Limit 1000000, Requested 7514243. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# create the index\n",
    "\n",
    "import os\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='D_ID',\n",
    "    node_label=\"District\",\n",
    "    text_node_properties= [\"Name\",\"Geometry\"], #['name', 'description', 'status'], #['name', 'state_capital', 'url'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5WQc6WAm2-M",
    "outputId": "631284c9-4ce1-4b98-b153-7a8480af5982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'index_for_state',\n",
       "  'type': 'VECTOR',\n",
       "  'labelsOrTypes': ['State'],\n",
       "  'properties': ['embedding'],\n",
       "  'options': {'indexProvider': 'vector-2.0',\n",
       "   'indexConfig': {'vector.hnsw.m': 16,\n",
       "    'vector.hnsw.ef_construction': 100,\n",
       "    'vector.dimensions': 1536,\n",
       "    'vector.similarity_function': 'COSINE',\n",
       "    'vector.quantization.enabled': True}}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the index just created\n",
    "vector_index.query(\n",
    "    \"\"\"SHOW INDEXES\n",
    "       YIELD name, type, labelsOrTypes, properties, options\n",
    "       WHERE type = 'VECTOR'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTBffq0fkvzR"
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "H-SFqvF3lw8B"
   },
   "outputs": [],
   "source": [
    "question1 = \"How many states in the database?\"\n",
    "question2 = \"How many geometries in the the database?\"\n",
    "question3 = \"What is the population of Hessen?\"\n",
    "question4 = \"What is the area of Hessen?\"\n",
    "question5 = \"What is the capital of Hessen?\"\n",
    "question6 = \"What is the geometry of Hessen?\"\n",
    "question7 = \"What are the geometries of Hessen and Niedersachsen?\"\n",
    "question8 = \"What is the url of the geometry of Hessen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUt4v84Tl0Ub",
    "outputId": "ed652ba9-1131-463a-a46d-b277ea320e16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nname: Hessen\\npopulation: 6391360\\nstate_capital: Wiesbaden\\narea: 21114.94'),\n",
       " Document(metadata={}, page_content='\\nname: Niedersachsen\\npopulation: 8140242\\nstate_capital: Hannover\\narea: 47709.82'),\n",
       " Document(metadata={}, page_content='\\nname: Nordrhein-Westfalen\\npopulation: 18139116\\nstate_capital: Düsseldorf\\narea: 34110.26'),\n",
       " Document(metadata={}, page_content='\\nname: Rheinland-Pfalz\\npopulation: 4159150\\nstate_capital: Mainz\\narea: 19854.21')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vector_index.similarity_search(question3)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sI3bBGOSl39j",
    "outputId": "baf88a3f-fcc6-4707-85be-b377350fbe50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content='\\nname: Hessen\\npopulation: 6391360\\nstate_capital: Wiesbaden\\narea: 21114.94'),\n",
       "  0.94384765625),\n",
       " (Document(metadata={}, page_content='\\nname: Niedersachsen\\npopulation: 8140242\\nstate_capital: Hannover\\narea: 47709.82'),\n",
       "  0.92498779296875),\n",
       " (Document(metadata={}, page_content='\\nname: Nordrhein-Westfalen\\npopulation: 18139116\\nstate_capital: Düsseldorf\\narea: 34110.26'),\n",
       "  0.9194488525390625),\n",
       " (Document(metadata={}, page_content='\\nname: Rheinland-Pfalz\\npopulation: 4159150\\nstate_capital: Mainz\\narea: 19854.21'),\n",
       "  0.917633056640625)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_with_score = vector_index.similarity_search_with_score(question3)\n",
    "response_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV4UuEw5kxaV"
   },
   "source": [
    "## Generation: Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "numPBbYwnWfr",
    "outputId": "15735836-3ab8-4b65-b72a-c3946c085e3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using documents as context\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "75euLZagnfn5"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PPOxD3ICnlFr"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "epdnd0jinoEP",
    "outputId": "e4724d29-47b5-42cd-dfe2-6e20d80d4725"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The population of Hessen is 6,391,360.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = response\n",
    "\n",
    "chain.invoke({\"context\": docs, \"question\": question3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPhwJ-NsocNt"
   },
   "source": [
    "## Generation: Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wMYQYPtOoakL",
    "outputId": "3b8c2b1f-bdff-44c3-d3d7-7f4c2591fc14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The population of Hessen is 6,391,360.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a retriever as context\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever = vector_index.as_retriever() # search_kwargs={\"k\": 1}\n",
    "\n",
    "graph_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "                )\n",
    "\n",
    "graph_chain.invoke(question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjYmkHlxtC9V"
   },
   "source": [
    "## Generation: Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "9tdhFRMxvgMM"
   },
   "outputs": [],
   "source": [
    "# Using a custom retriever as a context and post-processing of the answer\n",
    "# https://python.langchain.com/docs/how_to/custom_retriever/\n",
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\" Custom retriever to return the scores of the documents as well.\n",
    "        Then the scores are passed into an custom ranking function to include the spatial similarity\n",
    "        between the query and the document.\n",
    "    \"\"\"\n",
    "\n",
    "    vector_index: Neo4jVector\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "\n",
    "        docs, scores = zip(*self.vector_index.similarity_search_with_score(query))\n",
    "        for doc, score in zip(docs, scores):\n",
    "             print(\"***\", doc)\n",
    "             #new_score = updated_score(score, query, doc)\n",
    "             doc.page_content = doc.page_content\n",
    "             doc.metadata[\"score\"] = score\n",
    "        return docs\n",
    "\n",
    "def update_scores(docs):\n",
    "\n",
    "    for doc in docs:\n",
    "       new_score = doc.metadata[\"score\"] * 10\n",
    "       doc.page_content = doc.page_content+ \"\\nScore: \" + str(new_score)\n",
    "       doc.metadata[\"score\"] = new_score\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "fQqodttVtH5-",
    "outputId": "90fdda76-a330-4d5f-fbaf-4bbe771a418a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** page_content='\n",
      "name: Hessen\n",
      "population: 6391360\n",
      "state_capital: Wiesbaden\n",
      "area: 21114.94'\n",
      "*** page_content='\n",
      "name: Niedersachsen\n",
      "population: 8140242\n",
      "state_capital: Hannover\n",
      "area: 47709.82'\n",
      "*** page_content='\n",
      "name: Nordrhein-Westfalen\n",
      "population: 18139116\n",
      "state_capital: Düsseldorf\n",
      "area: 34110.26'\n",
      "*** page_content='\n",
      "name: Rheinland-Pfalz\n",
      "population: 4159150\n",
      "state_capital: Mainz\n",
      "area: 19854.21'\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The geometry of Hessen is 21114.94.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever_r = CustomRetriever(vector_index=vector_index)\n",
    "\n",
    "graph_chain = ({\"context\": retriever_r | update_scores, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "                )\n",
    "\n",
    "graph_chain.invoke(question6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQVWvsmwkq6f"
   },
   "source": [
    "# Cypher Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNXJS_B04j_"
   },
   "source": [
    "We will not dive deep into the cypher syntax during the course. The following queries should be enough for the interaction with the neo4j database. You can also check the [documentation](https://neo4j.com/docs/cypher-cheat-sheet/5/aura-dbe/auradb-free), if you happen to need more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POLTOnQ2j9Xt"
   },
   "outputs": [],
   "source": [
    "# delete every node and edge\n",
    "MATCH(n)\n",
    "DETACH DELETE (n)\n",
    "\n",
    "# create nodes and edges\n",
    "follow the structure shown at https://github.com/aurioldegbelo/sis2024/blob/main/vector_data/data.cypher\n",
    "\n",
    "# visualize the model of the graph database\n",
    "CALL apoc.meta.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzTwwHpcyOwi"
   },
   "source": [
    "# Project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63kwd8YDyuDb"
   },
   "source": [
    "* Exercice 01: clarify what your search target is\n",
    "\n",
    "* Exercice 02: elaborate on your data model (what are entities and relationships)\n",
    "\n",
    "* Exercice 03: create a neo4j account and a database instance\n",
    "\n",
    "* Exercice 04: create an example of cypher query (CREATE) for your data (just a few instances), upload it to the database to see if it works\n",
    "\n",
    "* Exercice 05: write a script to generate a CREATE query (it converts from your original format [csv, tsv, json, ...]) to a cypher template\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJqku/KwBmn8iEciiiYtpC",
   "collapsed_sections": [
    "Nv0FvC7kT3KR",
    "J-MZn6LV9qtB",
    "BV4UuEw5kxaV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
