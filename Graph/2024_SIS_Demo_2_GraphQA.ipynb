{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43I3IWmdbtN4"
   },
   "source": [
    "Topics: Data Modelling and Search Models\n",
    "* Langsmith (for inspection and debugging)\n",
    "* Semantic model extraction (continued)\n",
    "* Graph QA using GraphCypherQAChain\n",
    "* Graph QA using Vector Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsUHp7-VRbf8"
   },
   "source": [
    "# Chapter 1:  Langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbug8ocywoO7"
   },
   "source": [
    "[Documentation](https://docs.smith.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWn48jeBOlOA"
   },
   "source": [
    "[Website](https://www.langchain.com/langsmith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaMwdPwo6hVb",
    "outputId": "e32915be-98ed-406e-bbd6-93f564b3d7c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTM4jeoO7J7E",
    "outputId": "63363809-2fe8-4070-8777-278e728f7398"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA5Nm88y6ovr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YourKey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgGWbFpq7fGw",
    "outputId": "d9902e25-16d5-4286-9102-e675298f9b62"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vrjyoGMWSxFI",
    "outputId": "e3a0cd82-e78d-4875-f861-4ab2340dcc67"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser() # https://www.restack.io/docs/langchain-knowledge-langchain-stroutputparser-guide\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "question = \"What are the place names and geopolitical entities mentioned in the context?\"\n",
    "context = \"Germany is a country in Europe and its capital is Berlin.\"\n",
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv0FvC7kT3KR"
   },
   "source": [
    "# Chapter 2: Semantic Model Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2yaxk974TjZ",
    "outputId": "33bfca12-050e-47c6-f9c4-06345e4513ed"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain-community langchain-openai langchain_experimental neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwOmWv9h4ZAN"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+s://f02e0524.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"w60PF-SK2gGIlDII6zZMw8XMo67mqIFSrPU54_E3AU4\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kc7B0jrCvAZO"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZePyU65vKrD"
   },
   "outputs": [],
   "source": [
    "# From wikipedia: https://en.wikipedia.org/wiki/M%C3%BCnster\n",
    "example_text = \"\"\"\n",
    "Münster is an independent city (Kreisfreie Stadt)\n",
    "in North Rhine-Westphalia, Germany. It is in the northern part of the state and is considered to\n",
    " be the cultural centre of the Westphalia region. It is also a state district capital. Münster was the\n",
    "  location of the Anabaptist rebellion during the Protestant Reformation and the site of the signing of the\n",
    "   Treaty of Westphalia ending the Thirty Years' War in 1648. Today, it is known as the bicycle capital of Germany.\n",
    "Münster gained the status of a Großstadt (major city) with more than 100,000 inhabitants in 1915.[4]\n",
    " As of 2014, there are 300,000[5] people living in the city, with about 61,500 students,[6]\n",
    " only some of whom are recorded in the official population statistics as having their primary residence in Münster.\n",
    " Münster is a part of the international Euregio region with more than 1,000,000 inhabitants (Enschede, Hengelo, Gronau, Osnabrück).\n",
    " Companies offering jobs in Münster include the Institute for Geoinformatics at the University of Münster,\n",
    " the Münster University of Applied Sciences, Reedu GmbH, con terra, the Deutsche Bank, IKEA, LIDL, REWE, ALDI and BASF Coatings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO9j2tbdvPIc"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\") # https://platform.openai.com/docs/models\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm) # documentation, see https://python.langchain.com/docs/how_to/graph_constructing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYnlGBY8vTVD",
    "outputId": "b04410a5-41cc-4969-ab43-9ce3856dd6f1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=example_text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dHnhXk84n9h"
   },
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-MZn6LV9qtB"
   },
   "source": [
    "# Chapter 3: Graph QA using GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaQFfQ539seN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install  --quiet langchain langchain-openai langchain-community neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GolFRbIT98CE"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+ssc://f02e0524.databases.neo4j.io:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oO3-1C_9iV47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ORmHBZE89_9z"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"), # gpt-4o-mini\tgpt-3.5-turbo\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Cypher examples:\n",
    "# Which cities lie left of Münster?\n",
    "MATCH p=(C:City WHERE C.Name = \"Münster\") -[T:touches WHERE T.`Rel_Position`in [\"western\",\"southwestern\",\"northwestern\"]]->(:City) RETURN p\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"), # gpt-4o-mini\tgpt-3.5-turbo\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MlmganG9yec",
    "outputId": "c8faf5af-0a4a-4117-b9ad-89f24f5e273a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH p=(C:City WHERE C.Name = \"Münster\") -[T:touches]->(H:City WHERE H.Name = \"Havixbeck\") RETURN T.`Rel_Position`\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'T.`Rel_Position`': 'western'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'In what cardinal direction is havixbeck from Münster?',\n",
       " 'result': 'Havixbeck is located in the western cardinal direction from Münster.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_1 = \"What is the population of Hessen?\"\n",
    "question_2 = \"What is the geometry of Rheinland-Pfalz?\"\n",
    "question_3 = \"What are the areas of Hessen and Niedersachen. Is the area of Hessen bigger than the area of Niedersachsen\"\n",
    "question_4 = \"Is Düsseldorf the state capital of Nordrhein-Westfalen\"\n",
    "question_5 = \"Which cities lie in the district of Steinfurt?\"\n",
    "question_6 = \"Which cities lie southern, southeastern and southwestern of Münster? The relative position is saved as a property in the touches relation. Also give me every touches relation bewteen those cities.\"\n",
    "question_7 = \"Which cities lie within Steinfurt? Only return the Names.\"\n",
    "question_8 = \"Which cities lie right of Siegburg?\"\n",
    "question_9 = \"Which is the next bigger City with a population more than 500000 in the area of Bocholt\"\n",
    "question_10 = \"Has Köln or Düsseldorf a bigger population?\"\n",
    "question_11 = \"In what cardinal direction is havixbeck from Münster?\"\n",
    "question_12 = \"Was liegt rechts Bocholt?\"\n",
    "\n",
    "chain.invoke(question_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fva46S3Zb8Rs"
   },
   "source": [
    "# Chapter 4: GraphQA using Vector Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rivtxf2dkuLF"
   },
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWZcQcLRlMPb",
    "outputId": "be55d0fd-f97f-4c91-a2f7-08fec169ecea"
   },
   "outputs": [],
   "source": [
    "!pip install langchain openai wikipedia tiktoken neo4j langchain_openai langchain_community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb4QYjYElRAq"
   },
   "outputs": [],
   "source": [
    "# https://neo4j.com/developer-blog/knowledge-graph-rag-application/\n",
    "# https://github.com/tomasonjo/blogs/blob/master/llm/devops_rag.ipynb\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"neo4j+s://9df9a03f.databases.neo4j.io:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"MgDR4X6UnRMmXoJ-awLtSZKZkzY43jKpUuqnZKlnqn0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afOIDKPImj_V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlWANUOz0C6e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbWa90lejoVi"
   },
   "outputs": [],
   "source": [
    "# create the index\n",
    "\n",
    "import os\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='D_ID',\n",
    "    node_label=\"District\",\n",
    "    text_node_properties= [\"Name\",\"Geometry\"], #['name', 'description', 'status'], #['name', 'state_capital', 'url'],\n",
    "    embedding_node_property='embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5WQc6WAm2-M",
    "outputId": "631284c9-4ce1-4b98-b153-7a8480af5982"
   },
   "outputs": [],
   "source": [
    "# see the index just created\n",
    "vector_index.query(\n",
    "    \"\"\"SHOW INDEXES\n",
    "       YIELD name, type, labelsOrTypes, properties, options\n",
    "       WHERE type = 'VECTOR'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTBffq0fkvzR"
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-SFqvF3lw8B"
   },
   "outputs": [],
   "source": [
    "question1 = \"How many states in the database?\"\n",
    "question2 = \"How many geometries in the the database?\"\n",
    "question3 = \"What is the population of Hessen?\"\n",
    "question4 = \"What is the area of Hessen?\"\n",
    "question5 = \"What is the capital of Hessen?\"\n",
    "question6 = \"What is the geometry of Hessen?\"\n",
    "question7 = \"What are the geometries of Hessen and Niedersachsen?\"\n",
    "question8 = \"What is the url of the geometry of Hessen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUt4v84Tl0Ub",
    "outputId": "ed652ba9-1131-463a-a46d-b277ea320e16"
   },
   "outputs": [],
   "source": [
    "response = vector_index.similarity_search(question3)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sI3bBGOSl39j",
    "outputId": "baf88a3f-fcc6-4707-85be-b377350fbe50"
   },
   "outputs": [],
   "source": [
    "response_with_score = vector_index.similarity_search_with_score(question3)\n",
    "response_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV4UuEw5kxaV"
   },
   "source": [
    "## Generation: Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "numPBbYwnWfr",
    "outputId": "15735836-3ab8-4b65-b72a-c3946c085e3e"
   },
   "outputs": [],
   "source": [
    "# using documents as context\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75euLZagnfn5"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPOxD3ICnlFr"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "epdnd0jinoEP",
    "outputId": "e4724d29-47b5-42cd-dfe2-6e20d80d4725"
   },
   "outputs": [],
   "source": [
    "docs = response\n",
    "\n",
    "chain.invoke({\"context\": docs, \"question\": question3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPhwJ-NsocNt"
   },
   "source": [
    "## Generation: Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wMYQYPtOoakL",
    "outputId": "3b8c2b1f-bdff-44c3-d3d7-7f4c2591fc14"
   },
   "outputs": [],
   "source": [
    "# Using a retriever as context\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever = vector_index.as_retriever() # search_kwargs={\"k\": 1}\n",
    "\n",
    "graph_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "                )\n",
    "\n",
    "graph_chain.invoke(question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjYmkHlxtC9V"
   },
   "source": [
    "## Generation: Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tdhFRMxvgMM"
   },
   "outputs": [],
   "source": [
    "# Using a custom retriever as a context and post-processing of the answer\n",
    "# https://python.langchain.com/docs/how_to/custom_retriever/\n",
    "from typing import List\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\" Custom retriever to return the scores of the documents as well.\n",
    "        Then the scores are passed into an custom ranking function to include the spatial similarity\n",
    "        between the query and the document.\n",
    "    \"\"\"\n",
    "\n",
    "    vector_index: Neo4jVector\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "\n",
    "        docs, scores = zip(*self.vector_index.similarity_search_with_score(query))\n",
    "        for doc, score in zip(docs, scores):\n",
    "             print(\"***\", doc)\n",
    "             #new_score = updated_score(score, query, doc)\n",
    "             doc.page_content = doc.page_content\n",
    "             doc.metadata[\"score\"] = score\n",
    "        return docs\n",
    "\n",
    "def update_scores(docs):\n",
    "\n",
    "    for doc in docs:\n",
    "       new_score = doc.metadata[\"score\"] * 10\n",
    "       doc.page_content = doc.page_content+ \"\\nScore: \" + str(new_score)\n",
    "       doc.metadata[\"score\"] = new_score\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "fQqodttVtH5-",
    "outputId": "90fdda76-a330-4d5f-fbaf-4bbe771a418a"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retriever_r = CustomRetriever(vector_index=vector_index)\n",
    "\n",
    "graph_chain = ({\"context\": retriever_r | update_scores, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "                )\n",
    "\n",
    "graph_chain.invoke(question6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQVWvsmwkq6f"
   },
   "source": [
    "# Cypher Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNXJS_B04j_"
   },
   "source": [
    "We will not dive deep into the cypher syntax during the course. The following queries should be enough for the interaction with the neo4j database. You can also check the [documentation](https://neo4j.com/docs/cypher-cheat-sheet/5/aura-dbe/auradb-free), if you happen to need more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POLTOnQ2j9Xt"
   },
   "outputs": [],
   "source": [
    "# delete every node and edge\n",
    "MATCH(n)\n",
    "DETACH DELETE (n)\n",
    "\n",
    "# create nodes and edges\n",
    "follow the structure shown at https://github.com/aurioldegbelo/sis2024/blob/main/vector_data/data.cypher\n",
    "\n",
    "# visualize the model of the graph database\n",
    "CALL apoc.meta.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzTwwHpcyOwi"
   },
   "source": [
    "# Project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63kwd8YDyuDb"
   },
   "source": [
    "* Exercice 01: clarify what your search target is\n",
    "\n",
    "* Exercice 02: elaborate on your data model (what are entities and relationships)\n",
    "\n",
    "* Exercice 03: create a neo4j account and a database instance\n",
    "\n",
    "* Exercice 04: create an example of cypher query (CREATE) for your data (just a few instances), upload it to the database to see if it works\n",
    "\n",
    "* Exercice 05: write a script to generate a CREATE query (it converts from your original format [csv, tsv, json, ...]) to a cypher template\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJqku/KwBmn8iEciiiYtpC",
   "collapsed_sections": [
    "Nv0FvC7kT3KR",
    "J-MZn6LV9qtB",
    "BV4UuEw5kxaV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
